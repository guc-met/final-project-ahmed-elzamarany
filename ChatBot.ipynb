{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T14:32:08.378543Z",
     "iopub.status.busy": "2021-06-22T14:32:08.378079Z",
     "iopub.status.idle": "2021-06-22T14:32:08.384576Z",
     "shell.execute_reply": "2021-06-22T14:32:08.383091Z",
     "shell.execute_reply.started": "2021-06-22T14:32:08.378506Z"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T14:32:13.561105Z",
     "iopub.status.busy": "2021-06-22T14:32:13.560747Z",
     "iopub.status.idle": "2021-06-22T14:32:15.357037Z",
     "shell.execute_reply": "2021-06-22T14:32:15.356063Z",
     "shell.execute_reply.started": "2021-06-22T14:32:13.561073Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T14:32:15.394579Z",
     "iopub.status.busy": "2021-06-22T14:32:15.394246Z",
     "iopub.status.idle": "2021-06-22T14:32:23.689487Z",
     "shell.execute_reply": "2021-06-22T14:32:23.688128Z",
     "shell.execute_reply.started": "2021-06-22T14:32:15.394541Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "def LemTokens(tokens):\n",
    "       return [lemmer.lemmatize(token,'v') for token in tokens if token not in set(stopwords.words('english'))]\n",
    "def LemNormalize(text):\n",
    "       return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
    "def LemAllTokens(tokens):\n",
    "       return [lemmer.lemmatize(token,'v') for token in tokens]\n",
    "def LemAllNormalize(text):\n",
    "       return LemAllTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSentiment(sentence):\n",
    "    loaded_model = pickle.load(open('SentimentModel.sav', 'rb'))\n",
    "    Countvectorizer=pickle.load(open('Countvectorizer.pickle', 'rb'))\n",
    "    indeces_to_delete=pickle.load(open('indeces_to_delete.pickle', 'rb'))\n",
    "    getReady()\n",
    "    X_new = Countvectorizer.transform(sentence)\n",
    "    X_new_simple = np.delete(X_new.toarray(), indeces_to_delete, axis=1)\n",
    "    return loaded_model.predict(X_new_simple)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## this for using api to translate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put the credeintials in a file called service_account.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Translates text into the target language.\n",
    "\n",
    "Target must be an ISO 639-1 language code.\n",
    "See https://g.co/cloud/translate/v2/translate-reference#supported_languages\n",
    "\"\"\"\n",
    "from google.cloud import translate_v2 as translate\n",
    "def translate_text(target, text):\n",
    "\n",
    "\n",
    "    translate_client = translate.Client.from_service_account_json(\n",
    "        'service_account.json')\n",
    "\n",
    "\n",
    "\n",
    "    # Text can also be a sequence of strings, in which case this method\n",
    "    # will return a sequence of results for each text.\n",
    "    result = translate_client.translate(text, target_language=target)\n",
    "\n",
    "    return result[\"translatedText\"]\n",
    "#     print(u\"Text: {}\".format(result[\"input\"]))\n",
    "#     print(u\"Translation: {}\".format(result[\"translatedText\"]))\n",
    "#     print(u\"Detected source language: {}\".format(result[\"detectedSourceLanguage\"]))\n",
    "\n",
    "# translate_text('en', \"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "greeting_inputs = (\"اهلا\",\"صباح الخير\", \"مساء الخير\")\n",
    "# greeting_responses = [\"hey\",\"You seems happy? Is that true?\", \"hey hows you?\", \"*nods*\", \"hello, how you doing\", \"hello\", \"Welcome, I am good and you?\"]\n",
    "goodbyes_responses = [\"علي الرحب والسعه\",'شكرا','شكرا جزيلا', 'شكرا لك', \"مع السلامه\",\"الي اللقاء\"]\n",
    "goodbyes_inputs = ('شكرا', 'السلامه','اللقاء')\n",
    "sentiments={'sad':'حزين', 'afraid':\"خائف قليلا\", \"confident\":\"واثق من نفسك\",\"surprised\":\"متفاجئ قليلا\"}\n",
    "# def generate_greeting_response(greeting):\n",
    "#     for token in greeting.split():\n",
    "#         if token.lower() in greeting_inputs:\n",
    "#             return random.choice(greeting_responses)\n",
    "def generate_goodbyes_response(greeting):\n",
    "    for token in greeting.split():\n",
    "        if token.lower() in goodbyes_inputs:\n",
    "            return random.choice(goodbyes_responses)\n",
    "def wrong_sentiment(greeting):\n",
    "    for token in greeting.split():\n",
    "        if token.lower() ==\"لا\":\n",
    "            return True\n",
    "def mapsentiment(sentiment):\n",
    "    return sentiments[sentiment]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T17:46:14.414559Z",
     "iopub.status.busy": "2021-06-22T17:46:14.414149Z",
     "iopub.status.idle": "2021-06-22T17:46:14.433097Z",
     "shell.execute_reply": "2021-06-22T17:46:14.432090Z",
     "shell.execute_reply.started": "2021-06-22T17:46:14.414526Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_response(user_input,name):\n",
    "    word_vectorizer = pickle.load(open(name+'_word_vectorizer.sav', 'rb'))\n",
    "    all_word_vectors=pickle.load(open(name+'_all_word_vectors.pickle', 'rb'))\n",
    "    utterances=pickle.load(open(name+'_utterance.pickle', 'rb'))\n",
    "    prompts_sentences=pickle.load(open(name+'_prompts_sentences.pickle', 'rb'))\n",
    "    translated_utterance=pickle.load(open(name+'_translated_utterance.pickle', 'rb'))\n",
    "    tennisrobo_response = ''\n",
    "    user_input_vector= word_vectorizer.transform([user_input])\n",
    "    similar_prompt_vector_values = cosine_similarity(user_input_vector[0], all_word_vectors)\n",
    "    similar_sentence_number = similar_prompt_vector_values.argsort()[0][-1]\n",
    "                                      \n",
    "    matched_vector = similar_prompt_vector_values.flatten()\n",
    "    matched_vector.sort()\n",
    "    vector_matched = matched_vector[-1]\n",
    "    if vector_matched != 0 and similar_sentence_number< len(utterances)-1:\n",
    "        first = '' + utterances[similar_sentence_number+1]\n",
    "        translated_first = '' + translated_utterance[similar_sentence_number+1]\n",
    "        similar_sentence_second_number = similar_prompt_vector_values.argsort()[0][-2]\n",
    "        second = '' + utterances[similar_sentence_second_number+1]\n",
    "        translated_second = '' + translated_utterance[similar_sentence_second_number+1]\n",
    "        similar_sentence_third_number = similar_prompt_vector_values.argsort()[0][-3]\n",
    "        third = '' + utterances[similar_sentence_third_number+1]\n",
    "        translated_third = '' + translated_utterance[similar_sentence_third_number+1]\n",
    "#         print(\"first : \",first)\n",
    "#         print(matched_vector[-1])\n",
    "#         print(\"second : \",second)\n",
    "#         print(matched_vector[-2])\n",
    "#         print(\"third : \",third)\n",
    "#         print(matched_vector[-3])\n",
    "        Allreplies=[first,second,third]\n",
    "        translated_Allreplies=[translated_first,translated_second,translated_third]\n",
    "        Allprompts=[prompts_sentences[similar_sentence_number],prompts_sentences[similar_sentence_second_number],prompts_sentences[similar_sentence_third_number]]\n",
    "        Countvectorizer = CountVectorizer(tokenizer = LemAllNormalize, ngram_range=(1,1))\n",
    "        Countvectorizer.fit(Allprompts)\n",
    "        counts_vector= Countvectorizer.transform(Allprompts)\n",
    "        user_vector= Countvectorizer.transform([user_input])\n",
    "        count_prompt_cosine_similarity = cosine_similarity(user_vector, counts_vector)\n",
    "#         print(counts_vector[0])\n",
    "        count_matched_vector = count_prompt_cosine_similarity.flatten()\n",
    "        count_matched_vector.sort()\n",
    "#         fi=count_prompt_cosine_similarity.argsort()[0][0]\n",
    "#         print(Allprompts[fi],Allreplies[fi])\n",
    "#         print(count_matched_vector[0])\n",
    "#         si=count_prompt_cosine_similarity.argsort()[0][1]\n",
    "#         print(Allprompts[si],Allreplies[si])\n",
    "#         print(count_matched_vector[1])\n",
    "#         ti=count_prompt_cosine_similarity.argsort()[0][2]\n",
    "#         print(Allprompts[ti],Allreplies[ti])\n",
    "#         print(count_matched_vector[2])\n",
    "#         foi=count_prompt_cosine_similarity.argsort()[0][3]\n",
    "# #         print(Allprompts[foi],Allreplies[foi])\n",
    "# #         print(count_matched_vector[3])\n",
    "        return translate_text('ar', Allreplies[count_prompt_cosine_similarity.argsort()[0][-1]])\n",
    "    else:\n",
    "        return\" لا اعرف ماذا اقول ولكني اعرف انك \"+ mapsentiment(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run this for using the chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "فينتي: مرحبًا ، أنا صديقك فينتي. هل تريد التحدث معي قليلا؟ \n",
      "لقد عقدت العزيمه علي ان افقد وزني\n",
      "فينتي :  يبدو أنك واثق من نفسك هل هذا صحيح؟\n",
      "نعم\n",
      "فينتي: حسنا ، هل تريد التحدث اكتر عن ذلك؟ \n",
      "انا حقا اريد ان افقد وزني\n",
      "فينتي:  يمكنك. سوف يستغرق بعض المثابرة. كيف تسير الأمور حتى الآن؟\n",
      "امارس الرياضه بانتظام واظن انني استطيع ان افقد وزني\n",
      "فينتي:  أنا متأكد من أنه يمكنك فقط التفاني والجهد هو كل شيء!\n",
      "هذا صحيح شكرا لتشجيعك\n",
      "فينتي : علي الرحب والسعه\n"
     ]
    }
   ],
   "source": [
    "continue_dialogue = True\n",
    "sentiment=None\n",
    "print(\"فينتي: مرحبًا ، أنا صديقك فينتي. هل تريد التحدث معي قليلا؟ \")\n",
    "while(continue_dialogue == True):\n",
    "    \n",
    "    human_text = input()\n",
    "    human_text = human_text.lower()\n",
    "    \n",
    "    if human_text != 'bye':\n",
    "        if generate_goodbyes_response(human_text) != None:\n",
    "            continue_dialogue = False\n",
    "            print(\"فينتي :\" , generate_goodbyes_response(human_text))\n",
    "        else:\n",
    "            human_text=translate_text('en', human_text)\n",
    "            if sentiment == None:\n",
    "                sentiment= getSentiment([human_text])\n",
    "                print(\"فينتي : \",\"يبدو أنك \"+mapsentiment(sentiment)+\" هل هذا صحيح؟\")\n",
    "                human_text = input()\n",
    "                human_text = human_text.lower()\n",
    "                human_text=translate_text('en', human_text)\n",
    "                print(\"فينتي: حسنا ، هل تريد التحدث اكتر عن ذلك؟ \")\n",
    "                human_text = input()\n",
    "                human_text = human_text.lower()\n",
    "                human_text=translate_text('en', human_text)\n",
    "                if wrong_sentiment(human_text)==True:\n",
    "                    sentiment= getSentiment([human_text])\n",
    "#                 print(\"typing...\")\n",
    "                print(\"فينتي: \", generate_response(human_text,sentiment))\n",
    "            else:\n",
    "#                 print(\"typing...\")\n",
    "                print(\"فينتي: \", generate_response(human_text,sentiment))\n",
    "    else:\n",
    "        continue_dialogue = False\n",
    "        print(\"فينتي: \",\"مع السلامه اعتني بنفسك جيدا\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
